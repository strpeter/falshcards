\documentclass[
			print,
			a6paper,
			%flip,
			grid=none]{kartei}

\usepackage[utf8]{inputenc} %UTF8
\usepackage[T1]{fontenc}
\usepackage[scaled]{helvet}
\usepackage[english]{babel}
%
\usepackage{graphicx}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{array} %for tabular (figure side by side text)
\usepackage{mathtools}
\usepackage{cases} %allows to separate different cases ;)
%\usepackage{color}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor} %This is the package, you ment.
\usepackage{bbm} % provides nice formatting of C,R,Q,id, etc.
%
% Some additional stuff:
\input{macros}
%
%
%
%
%
% Begin document:
\begin{document}
%
%
% Here now the interesting part:
% Remember that it is essential to set appropriate CARDSUBJECT,COMMENT infront of each single flashcard!!!
%
%......................................................
% Section 2.2.
\comment{Section 2.2}
%......................................................
%
\begin{karte}{Definition: Bayesian point of view}
	XXX
\end{karte}

\begin{karte}{Definition: Alphabet}
	An \emph{alphabet} $\cX$ is the set of outcomes after a single measurement or random trial.
\end{karte}

\begin{karte}{Definition: Sample space}
	A \emph{sample space} $\Omega$ is the set of all outcomes of some random trial or experiment of several measurements. We write usually $\Omega=\cX^n$ with $n\in\bbN$.
\end{karte}

\begin{karte}{Definition and Properties: Sigma-algebra, events}
	A \emph{$\sigma$-algebra} $\cE$ is a collection of sets on the sample space but contains at least one element. The elements $E\in\cE$ are called \emph{events}. All $\sigma$-algebras are subsets of the power set $\mathcal{P}(\Omega)$.
\end{karte}

\begin{karte}{Definition: Probability measure, Probability of an event}
	A \emph{probability measure} $P$ on $(\Omega, \cE)$ is  a function 
	\begin{equation*}
		P: \cE \to [0,1], E\mapsto P[E],
	\end{equation*}
	that satisfies the probability axioms
	\begin{itemize}
		\item $P[\Omega]=1$
		\item $P[\bigcup_{i\in\mathbbm{N}}E_i]=\sum_{i\in\mathbbm{N}}P[E_i]$ for any family $(E_i)_{i\in\mathbbm{N}}$ of pairwise disjoint events.
	\end{itemize}
	The non-negative real numbers $P[E]$ are called the \emph{probability of the event E}.
\end{karte}

\begin{karte}{Definition: Measurable function}
	A function $f:\Omega\to\cX$ is called \emph{measurable with respect to the $\sigma$-algebras $\cE$ and $\mathcal{F}$} $\iff$ the preimage of any $F\in\mathcal{F}$ is an event ($X^{-1}(F)\in\cE$).
\end{karte}

\begin{karte}{Definition: Probability space}
	A \emph{probability space} $(\Omega, \cE, P)$ is a triple consisting of 
	\begin{itemize}
		\item the sample space $\Omega$ -- an arbitrary non-empty set,
		\item the $\sigma$-algebra $\cE \subseteq 2^{\Omega}$ -- a set of subsets of $\Omega$(containing the empty set), called events,
		\item the probability measure $P: \cE\to [0,1]$ -- a function on $\cE$ such that $P[\Omega]=1$,
	\end{itemize}
	where $(\Omega, \cE)$ is a measurable space.
\end{karte}

\begin{karte}{Definition: Random variable}
	A \emph{random variable} $X$ is a function 
	\begin{equation*}
		\Omega \to X(\Omega):=\mathcal{X}, \omega\mapsto x,
	\end{equation*}
	which is measurable with respect to the $\sigma$-algebras $\cE$,$\mathcal{F}$.
\end{karte}

\begin{karte}{Definition: Range of a random variable}
	The \emph{range of a random variable $X$} is the measurable space $(\cX,\mathcal{F})$.
\end{karte}

\begin{karte}{Definition: Joint probability measure}
	A \emph{joint probability measure on $(\cX\times\mathcal{Y},\mathcal{F}\times\mathcal{G})$} induced by a pair of random variables $(X,Y):\Omega\to\cX\times\mathcal{Y},\omega\mapsto X(\omega)\times Y(\omega)$, where $X$ and $Y$ have the range $(\cX,\mathcal{F})$ and $(\mathcal{Y},\mathcal{G})$, is denoted by $P_{XY}$. \\ Comment: This definition can be extended to an arbitrary amount of random variables.
\end{karte}

\begin{karte}{Definition: Conditioned probability}
	A \emph{probability of $E$ conditioned on $E'$} is 
	\begin{equation*}
		P[E\vert E']:=\frac{P[E\cap E']}{P[E']},\quad \textrm{for } E,E'\in\cE.
	\end{equation*}
\end{karte}

\begin{karte}{Definition: Mutually independence}
	Two events $E,E'\in\cE$ are called \emph{mutually independent} $\iff P[E\cap E']=P[E]P[E']$. \\ $P[E\vert E']=P[E]$ if $E$ and $E'$ are mutually independent.
\end{karte}

\begin{karte}{Definition: Probability mass function}
	A \emph{probability mass function} $P_X$ is a function
	\begin{equation*}
		P_X: \cX\to\bbR, x\mapsto P_X(x):=\sum_{\omega\in X^{-1}(x)}P[\omega].
	\end{equation*}
	In the script at p. 6, I get the impression, as if the probability mass function were only defined for discrete random variables. What is then the probability distribution???
\end{karte}

\begin{karte}{Proposition: Normalization condition}
	The normalization condition for a probability mass function $P_X$ is
	\begin{equation*}
		\sum_{x\in\cX} P_X(x)=1
	\end{equation*}
	\label{prop:normalization}
\end{karte}

\begin{karte}{Proposition: State of knowledge}
	The probability distribution interpreted as a state of knowledge is a subjective quantity. In the following, it is ment the state of knowledge of an outside observer.
\end{karte}

\begin{karte}{Definition: Probability measure of a random variable conditioned}
	A \emph{probability measure of a random variable $X$ conditioned on $E'$} is ??? XXX
\end{karte}

\begin{karte}{Definition: Probability mass function conditioned}
	A \emph{probability mass function of $X$ conditioned on $E'$} is given by
	\begin{equation*}
		P_{X\vert E'}(x):= P_{X\vert E'}[\{x\}]
	\end{equation*}
	and satisfies the normalization condition.
\end{karte}


%......................................................
% Section 2.3.
\comment{Section 2.3}
%......................................................

\begin{karte}{Definition: Marginal distributions}
	The distributions $P_X$ and $P_Y$ are called the \emph{marginal distributions} of the distribution $P_{XY}$. \\
	Comment: The marginal distribution can be written as $P_X (x) = \sum_{y\in\mathcal{Y}} P_{XY}(x,y)$.
\end{karte}

\begin{karte}{Proposition: Conditioned distribution}
	The distribution $P_{X\vert Y=y}$ of X conditioned on the event $Y=y$ obeys
	\begin{equation*}
		P_{X\vert Y=y}(x) = \frac{P_{XY}(x,y)}{P_Y(y)} 	\quad	\forall x\in\cX
	\end{equation*}
\end{karte}

\begin{karte}{Definition: Support}
	The \emph{support of the function $P_X$} is defined as 
	\begin{equation*}
		\supp{P_X}:=\{x\in\cX :P_X(x)\neq 0\}.
	\end{equation*}
\end{karte}

\begin{karte}{Definition: Uniform distribution}
	A distribution $P_X$ is \emph{uniform} $\iff P_X(x)=\frac{1}{\lvert\cX\rvert}	\quad	\forall x\in\cX$.
\end{karte}

\begin{karte}{Definition: Flat distribution}
	A distribution $P_X$ is \emph{flat} $\iff \exists~ q\in[0,1]:\quad P_X(x)\in{0,q}	\quad	\forall x\in\cX$.
\end{karte}

\begin{karte}{Definition: Markov chain}
	A sequence of random variables $X_1,\ldots,X_n$ has the \emph{Markov property} or is called \emph{Markov chain} (denoted $X_1\leftrightarrow\ldots\leftrightarrow X_n$)
	\begin{equation*}
		\iff P_{X_{i+1}\vert X_1=x_1,\ldots,X_i=x_i} = P_{X_{i+1}\vert X_i=x_i} \quad	\forall x_1,\ldots, x_i, \forall i\in\{1,\dots,n-1\}.
	\end{equation*}
\end{karte}

\begin{karte}{Proposition: Probability mass function of a function}
	Let $X$ be a random variable with alphabet $\cX$ and let $f$ be a function $f:\cX\to\mathcal{Y}$. Therefore, $f(X)=f\circ X$ is again a random variable which has alphabet $\mathcal{Y}$ and the corresponding probability mass function is 
	\begin{equation*}
		P_{f(X)}(y) = \sum_{x\in f^{-1}(\left\{ y \right\})} P_X(x)
	\end{equation*}
\end{karte}

\begin{karte}{Definition: Expectation value}
	The \emph{expectation value of a random variable $X$} is defined as
	\begin{equation*}
		\avg{X}_{P_X}:=\sum_{x\in X} P_X(x).
	\end{equation*}
\end{karte}

\begin{karte}{Proposition: Jensen inequality}
	\emph{Jensen inequality:} For a convex real function $f$ on a convex set $\cX$, the expectation values of $X$ and $f(X)$ are related by
	\begin{equation*}
		\avg{f(X)}\geq f(\avg{X}).
	\end{equation*}
\end{karte}

\begin{karte}{Definition and Properities: Trace distance}
	The \emph{trace distance $\delta$ between $P$ and $Q$} is defined by
	\begin{equation*}
		\delta(P,Q) = \frac{1}{2}\sum_{x\in\cX} \abs{P(x) - Q(x)},
	\end{equation*}
	where $P,Q$ are two probability mass functions. It is also called \emph{statistical distance}, \emph{variational distance}, or \emph{Kolmogorov distance}.

	The trace distance is metric, i.e. it is symmetric, non-negative, zero $\iff P=Q$, and it satisfies the triangle inequality. The trace distance fullfills $\delta(P,Q)\leq 1 \;\forall P,Q$ and 
	\begin{equation*}
		\delta(P,Q)=1 \iff \supp{P\cap Q}=0.
	\end{equation*}
\end{karte}

\begin{karte}{Proposition: Normalization of the trace distance}
	The trace distance of $P$ and $Q$ satisfying the normalization condition in Prop. \ref{prop:normalization} can be written as
	\begin{equation*}
		\delta(P,Q) = 1-\sum_{x\in\cX} \min{[P(x) - Q(x)]}.
	\end{equation*}
\end{karte}

\begin{karte}{Lemma: Distance of marginals}
	Let $Q_X$ and $Q_{X'}$ be probability mass functions on $\cX$. 
	\begin{equation*}
		\implies \delta(Q_X,Q_{X'}) = \min_{P_{XX'}} P_{XX'}[X\neq X'],
	\end{equation*}
	where the minimum ranges over all joint probability mass functions $P_{XX'}$ with marginals $P_X=Q_X$ and $P_{X'}=Q_{X'}$.\\
	Comment: The trace distance can only decrease under operation of taking marginals.
\end{karte}

\begin{karte}{Lemma: Distance of joint probabilities}
	For any two probability (???) mass functions $P_{XY}$ and $Q_{XY}$,
	\begin{equation*}
		\delta(P_{XY},Q_{XY}) \geq \delta(P_X,Q_X).
	\end{equation*}
\end{karte}

\begin{karte}{Definition: i.i.d.}
	A $n$-tuple of random variables $X_1,\dots,X_n$ with alphabet $\cX$ is \emph{independent and identically distributed (i.i.d.)}  
	\begin{equation*}
		\iff P_{X_1\dots X_n} = P_X^{\times n} := P_X \times\dots\times P_X.
	\end{equation*}
	Comment: This definition is applied on processes that are repeated $n$ times independently, e.g. repeated useof a communication channel.
\end{karte}

\begin{karte}{Proposition: Markov's inequality}
	\emph{Markov's inequality.} Let X be a positive (why positive, can it also be negative???) random variable.
	\begin{equation*}
		\implies P[X\geq\varepsilon] \leq \frac{\avg{X}_P}{\varepsilon}.
	\end{equation*}
\end{karte}

\begin{karte}{Proposition: Law of large numbers}
	Let $X_1, X_n\dots$ be an infinite sequence of i.i.d. random variables $X$. The \emph{weak law of large numbers} predicts
	\begin{equation*}
		\lim_{n\to\infty} P\left[ (\avg{X} - \frac{1}{n}\sum_{i=1}^n X_i)^2 < \varepsilon \right] = 1 \quad \forall \varepsilon>0,
	\end{equation*}
	The \emph{strong law of large numbers} predicts
	\begin{equation*}
		P\left[ \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n X_i = \avg{X} \right] = 1
	\end{equation*}
	i.e. the sample mean converges to the expectation value of $X$ with probability $1$.
\end{karte}

\begin{karte}{Definition: Channel}
	A \emph{channel $\mathbf{p}$} is a probabilistic mapping 
	\begin{equation*}
		\mathbf{p}: \cX\times\mathcal{Y}\to\bbR^+, (x,y)\mapsto\mathbf{p}(y\vert x),
	\end{equation*}
	such that $P_Y(y)=\mathbf{P}(y\vert x) ~\forall x\in\cX$.
\end{karte}

\begin{karte}{Definition: Output random variable}
	A channel $\mathbf{p}: \cX\to\mathcal{Y}$ defines a new random variable $Y$ on the output alphabet $\mathcal{Y}$ via the joint probability mass function 
	\begin{equation*}
		P_{XY}(x,y) := P_X(x)\mathbf{p}(y\vert x).
	\end{equation*}
	Comment: A channel assigns to each value of an input alphabet $\cX$ a value of the output alphabet $\mathcal{Y}$. The concept of channels generalizes in this sense the concept of functions on alphabets. This concept describes also communications between a sender with input $X$ and a receiver with output $Y$.
\end{karte}


%......................................................
% Section 3.
\comment{Section 3}
%......................................................
% Section 3.1
\comment{Section 3.1}
%......................................................

\begin{karte}{What is an Entropy measure?}
	An \emph{Entropy measure} counts the amount of information or uncertainty, which is equivalent.
\end{karte}

\begin{karte}{Applications of data compression}
	Data trasmission and Data storage.
\end{karte}

\begin{karte}{Definition: Algorithmic Entropy}
	The \emph{algorithmic entropy of a message $X$} is defined as the minimum lenght of an algorithm that generates $X$. The \emph{Kolmogorov complexity} is a synonym for \emph{algorithmic entropy}.
\end{karte}

\begin{karte}{Definition: Kolmogorov complexity}
	The \emph{Kolmogorov complexity of a message $X$} is defined as the minimum lenght of an algorithm that generates $X$. The \emph{Kolmogorov complexity} is a synonym for \emph{algorithmic entropy}.
\end{karte}

\begin{karte}{Disadvantage of Kolmogorov complexity}
	The Kolmogorov complexity or algorithmic entropy is not computable in general.
\end{karte}

\begin{karte}{Definition: Entropy of an event}
	The \emph{entropy of an event $E\in (\Omega,\cE,P$} is a function 
	\begin{equation*}
		H:\cE\to\bbR\cup\{\infty\}, E\mapsto H(E)
	\end{equation*}
	with the following properties:
	\begin{itemize}
		\item	\emph{Independence of the representation:}\\ $H(E)$ only depends on the probability $P[E]$ of the event $E$,
		\item	\emph{Continuity:}\\ $H$ is continuous in the probability measure $P$ (relative to the topology induced by the trace distance),
		\item	\emph{Additivity:}\\ $H(E\cap E`)=H(E)+H(E`)$ for two independent events $E$ and $E`$,
		\item	\emph{Normalization:}\\ $H(E)=1$ for $E$ with $P[E]=\frac{1}{2}$.
	\end{itemize}

	Note: It is reasonable to think of $H$ as a measure of uncertainty.
\end{karte}
%
%
%
%
%
%
%
\end{document}
